{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80d88e8-b48a-425e-8167-b82acabc444b",
   "metadata": {},
   "source": [
    "# Reducción del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c98cef-ee56-4e34-8a0f-355ac5119708",
   "metadata": {},
   "source": [
    "En este notebook vamos a realizar una reducción del dataset original, donde explicamos las razones de esta decisión y el proceso llevado a cabo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c9541-c376-4f3d-80e1-584c7f0f489e",
   "metadata": {},
   "source": [
    "En la fase 1 del proyecto (Identificación y estudio teórico de datos industriales) analizamos la estructura del dataset *Tennessee Eastman Process (TEP)*. Este esta compuesto por simulaciones industriales que representan tanto el comportamiento normal como los 20 distintos tipos de fallos.\n",
    "\n",
    "En ese análisis detallamos las variables que forman parte del proceso, comprobamos que no había valores nulos, y confirmamos que el volumen de datos del dataset era enorme (cientos de miles de registros), especialmente para los archivos de fallos.\n",
    "\n",
    "Este tamaño es ideal para realizar una investigación avanzada, pero para desarrollar nuestro proyecto, nos supone grandes limitaciones. Por ejemplo, usar el dataste completo afecta en los tiempos de carga de los archivos, retrasa el entrenamiento y dificulta la experimentación del usuario en los paneles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0599529b-413f-4df0-8a29-21023f73b4c0",
   "metadata": {},
   "source": [
    "Por estas razones, antes de avanzar con los siguientes pasos del proyecto, entre ellos, realizar un análisis exploratorio más profundo, la ingeniería de características y creación de modelos, vamos a **reducir el dataset original**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d908170a-dd21-4c67-b554-632a6c3a3a88",
   "metadata": {},
   "source": [
    "#### Estrategia para la reducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596de332-6d71-45cd-8f6b-99dcfd100515",
   "metadata": {},
   "source": [
    "Para conservar el proceso industrial pero sin usar todas las simulaciones del dataset, vamos a usar un **muestreo a nivel de simulación**, estratificado por tipo de fallo en los dataset que tienen fallos, y muestreo aleatorio de simulaciones en los datasets sin fallos. De esta forma mantenemos el proceso completo, solo reducimos el número de simulaciones.\n",
    "\n",
    "Mantenemos todas la columnas originales del conjunto de datos original y los segmentos temporales (conservamos las simulaciones completas). Pero tambien creamos una nueva variable binaria, *fault_present*, para indicar si la simulación pertenece a un escenario de fallo o no. \n",
    "Para los datasets Faulty vamos a seleccionar 10 simulaciones por tipo de fallo y por otro lado, para los datasets de Fault-Free, vamos a seleccionar 20 simulaciones. Esto lo hacemos sin mezclar las condiciones, si el archivo solo tiene fallos, se muestrean solo simulaciones con fallo, y lo mismo en caso contrario.\n",
    "\n",
    "De esta forma garantizamos que los datos reducidos mantienen la dimensionalidad original (las 52 variables del proceso) y reflejamos ambas condiciones, las normales y con los fallos simulados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f17c7-4427-471d-ba24-f25ca14375f5",
   "metadata": {},
   "source": [
    "### Faulty_Training y Faulty_Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6040a18-7927-46d7-9b23-c34032dc6f4e",
   "metadata": {},
   "source": [
    "Comenzamos reduciendo los archivos que tienen las simulaciones con fallos.\n",
    "\n",
    "Primero cargamos los archivos .RData y convertimos las columnas de tipo float64 a float32, de esta forma reducimos memoria. Después creamos una etiqueta para indicar si hay fallo o no (fault_present). El siguiente paso es identificar la columna que define las simulaciones (simulationRun) y aplicamos el muestreo adecuado dependiendo de si el archivo tiene o no tiene fallos. Por último creamos y guardamos los nuevos archivos reducidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2671efee-df7f-459a-bd4d-2e96e486c0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Faulty Training reducido ---\n",
      "Archivo guardado en: C:\\Users\\elbab\\Documents\\ProyectoAnalitica\\proyecto_analitica\\DatasetReducido\\Faulty_Training_reduced.csv\n",
      "Simulaciones: 10\n",
      "Muestras: 100000\n",
      "Columnas: ['faultNumber', 'simulationRun', 'sample', 'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11', 'fault_present']\n",
      "\n",
      "--- Faulty Testing reducido ---\n",
      "Archivo guardado en: C:\\Users\\elbab\\Documents\\ProyectoAnalitica\\proyecto_analitica\\DatasetReducido\\Faulty_Testing_reduced.csv\n",
      "Simulaciones: 10\n",
      "Muestras: 192000\n",
      "Columnas: ['faultNumber', 'simulationRun', 'sample', 'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11', 'fault_present']\n",
      "\n",
      "Todos los datasets reducidos guardados en la carpeta DatasetReducido.\n"
     ]
    }
   ],
   "source": [
    "import pyreadr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_and_reduce(file_path, n_sims_per_fault=10, n_sims_faultfree=20):\n",
    "    try:\n",
    "        result = pyreadr.read_r(file_path)\n",
    "        df_name = list(result.keys())[0]\n",
    "        df = result[df_name]\n",
    "\n",
    "        # Pasar a float32\n",
    "        for col in df.select_dtypes(include=\"float64\").columns:\n",
    "            df[col] = df[col].astype(\"float32\")\n",
    "\n",
    "        # Crear etiqueta\n",
    "        df['fault_present'] = (df['faultNumber'] > 0).astype(int)\n",
    "\n",
    "        # Detectar columna de simulación\n",
    "        sim_col = \"simulationRun\"\n",
    "\n",
    "        # Comprobar si el dataset tiene fallos\n",
    "        fault_values = df[\"faultNumber\"].unique()\n",
    "        has_faults = any(fault_values > 0)\n",
    "\n",
    "        # CASO 1 — DATASET FAULTY\n",
    "        if has_faults:\n",
    "            df_faulty = df[df[\"faultNumber\"] > 0]\n",
    "\n",
    "            # muestrear n simulaciones por tipo de fallo\n",
    "            selected_faulty_sims = (\n",
    "                df_faulty.groupby(\"faultNumber\")[sim_col]\n",
    "                .apply(lambda sims: sims.drop_duplicates().sample(\n",
    "                    n=min(n_sims_per_fault, len(sims)), random_state=42))\n",
    "                .explode()\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            df_reduced = df[df[sim_col].isin(selected_faulty_sims)]\n",
    "\n",
    "            return df_reduced, sim_col\n",
    "\n",
    "        # CASO 2 — DATASET FAULT-FREE\n",
    "        else:\n",
    "            selected_sims = (\n",
    "                df[sim_col].drop_duplicates().sample(\n",
    "                    n=min(n_sims_faultfree, df[sim_col].nunique()),\n",
    "                    random_state=42\n",
    "                )\n",
    "            )\n",
    "\n",
    "            df_reduced = df[df[sim_col].isin(selected_sims)]\n",
    "            return df_reduced, sim_col\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR load_and_reduce:\", e)\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "\n",
    "# Ejemplo de uso completo\n",
    "DATA_DIR = os.getcwd()\n",
    "DEST_DIR = os.path.join(DATA_DIR, \"DatasetReducido\")\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "\n",
    "file_paths = {\n",
    "    \"Faulty Training\": os.path.join(DATA_DIR, \"TEP_Faulty_Training.RData\"),\n",
    "    \"Faulty Testing\": os.path.join(DATA_DIR, \"TEP_Faulty_Testing.RData\"),\n",
    "}\n",
    "\n",
    "for name, path in file_paths.items():\n",
    "    df_reduced, sim_col = load_and_reduce(path)\n",
    "    if not df_reduced.empty:\n",
    "        out_file = os.path.join(DEST_DIR, f\"{name.replace(' ', '_')}_reduced.csv\")\n",
    "        df_reduced.to_csv(out_file, index=False)\n",
    "        print(f\"\\n--- {name} reducido ---\")\n",
    "        print(\"Archivo guardado en:\", out_file)\n",
    "        print(\"Simulaciones:\", df_reduced[sim_col].nunique())\n",
    "        print(\"Muestras:\", len(df_reduced))\n",
    "        print(\"Columnas:\", df_reduced.columns.tolist())\n",
    "\n",
    "print(\"\\nTodos los datasets reducidos guardados en la carpeta DatasetReducido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270c7d4-6ba5-4ffa-abe8-22bd5ec1d43f",
   "metadata": {},
   "source": [
    "### FaultFree_Training y FaultFree_Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16064c6b-b60d-47fb-b4e2-35a9b67b22e9",
   "metadata": {},
   "source": [
    "Para estos otros dos archivos Fault-Free, que no contienen fallos usamos la misma función de reducción que utilizamos para los datasets con fallos. Como en este caso no hay distintos tipos de fallo, solo aplicamos una selección aleatoria para limitar el número de simulaciones. Cargamos los datasets, convertimos las columnas de tipo float64 a float32. De esta forma creamos un dataset reducido conservando las columnas originales y la estructura de las simulaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "074c3b67-3ddb-482a-99af-2d59d7820b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FaultFree Training reducido ---\n",
      "Archivo guardado en: C:\\Users\\elbab\\Documents\\ProyectoAnalitica\\proyecto_analitica\\DatasetReducido\\FaultFree_Training_reduced.csv\n",
      "Simulaciones: 20\n",
      "Muestras: 10000\n",
      "Columnas: ['faultNumber', 'simulationRun', 'sample', 'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11', 'fault_present']\n",
      "\n",
      "--- FaultFree Testing reducido ---\n",
      "Archivo guardado en: C:\\Users\\elbab\\Documents\\ProyectoAnalitica\\proyecto_analitica\\DatasetReducido\\FaultFree_Testing_reduced.csv\n",
      "Simulaciones: 20\n",
      "Muestras: 19200\n",
      "Columnas: ['faultNumber', 'simulationRun', 'sample', 'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11', 'fault_present']\n",
      "\n",
      "Todos los datasets reducidos guardados en la carpeta DatasetReducido.\n"
     ]
    }
   ],
   "source": [
    "import pyreadr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# def load_and_reduce(file_path, n_sims_per_fault=10, n_sims_faultfree=20):\n",
    "#     try:\n",
    "#         result = pyreadr.read_r(file_path)\n",
    "#         df_name = list(result.keys())[0]\n",
    "#         df = result[df_name]\n",
    "\n",
    "#         # Pasar a float32\n",
    "#         for col in df.select_dtypes(include=\"float64\").columns:\n",
    "#             df[col] = df[col].astype(\"float32\")\n",
    "\n",
    "#         # Crear etiqueta\n",
    "#         df['fault_present'] = (df['faultNumber'] > 0).astype(int)\n",
    "\n",
    "#         # Detectar columna de simulación\n",
    "#         sim_col = \"simulationRun\"\n",
    "\n",
    "#         # Comprobar si el dataset tiene fallos\n",
    "    #     fault_values = df[\"faultNumber\"].unique()\n",
    "    #     has_faults = any(fault_values > 0)\n",
    "\n",
    "    #     # CASO 1 — DATASET FAULTY\n",
    "    #     if has_faults:\n",
    "    #         df_faulty = df[df[\"faultNumber\"] > 0]\n",
    "\n",
    "    #         # muestrear n simulaciones por tipo de fallo\n",
    "    #         selected_faulty_sims = (\n",
    "    #             df_faulty.groupby(\"faultNumber\")[sim_col]\n",
    "    #             .apply(lambda sims: sims.drop_duplicates().sample(\n",
    "    #                 n=min(n_sims_per_fault, len(sims)), random_state=42))\n",
    "    #             .explode()\n",
    "    #             .reset_index(drop=True)\n",
    "    #         )\n",
    "\n",
    "    #         df_reduced = df[df[sim_col].isin(selected_faulty_sims)]\n",
    "\n",
    "    #         return df_reduced, sim_col\n",
    "\n",
    "    #     # CASO 2 — DATASET FAULT-FREE\n",
    "    #     else:\n",
    "    #         selected_sims = (\n",
    "    #             df[sim_col].drop_duplicates().sample(\n",
    "    #                 n=min(n_sims_faultfree, df[sim_col].nunique()),\n",
    "    #                 random_state=42\n",
    "    #             )\n",
    "    #         )\n",
    "\n",
    "    #         df_reduced = df[df[sim_col].isin(selected_sims)]\n",
    "    #         return df_reduced, sim_col\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print(\"ERROR load_and_reduce:\", e)\n",
    "    #     return pd.DataFrame(), None\n",
    "\n",
    "# Ejemplo de uso completo\n",
    "DATA_DIR = os.getcwd()\n",
    "DEST_DIR = os.path.join(DATA_DIR, \"DatasetReducido\")\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "\n",
    "file_paths = {\n",
    "    \"FaultFree Training\": os.path.join(DATA_DIR, \"TEP_FaultFree_Training.RData\"),\n",
    "    \"FaultFree Testing\": os.path.join(DATA_DIR, \"TEP_FaultFree_Testing.RData\"),\n",
    "}\n",
    "\n",
    "for name, path in file_paths.items():\n",
    "    df_reduced, sim_col = load_and_reduce(path)\n",
    "    if not df_reduced.empty:\n",
    "        out_file = os.path.join(DEST_DIR, f\"{name.replace(' ', '_')}_reduced.csv\")\n",
    "        df_reduced.to_csv(out_file, index=False)\n",
    "        print(f\"\\n--- {name} reducido ---\")\n",
    "        print(\"Archivo guardado en:\", out_file)\n",
    "        print(\"Simulaciones:\", df_reduced[sim_col].nunique())\n",
    "        print(\"Muestras:\", len(df_reduced))\n",
    "        print(\"Columnas:\", df_reduced.columns.tolist())\n",
    "\n",
    "\n",
    "print(\"\\nTodos los datasets reducidos guardados en la carpeta DatasetReducido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ad1c80-bf0a-4aa7-9df5-9c850f16319f",
   "metadata": {},
   "source": [
    "Después de realizar este proceso de reducción de datos, vamos a comprobar los resultados cargando los datasets de training reducidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0112452b-f302-4029-aaf2-11b4e911bf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAULTY:\n",
      "fault_present\n",
      "1    100000\n",
      "Name: count, dtype: int64\n",
      "Simulaciones únicas: 10\n",
      "\n",
      "FAULTFREE:\n",
      "fault_present\n",
      "0    10000\n",
      "Name: count, dtype: int64\n",
      "Simulaciones únicas: 20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_faulty = pd.read_csv(\"DatasetReducido/Faulty_Training_reduced.csv\")\n",
    "df_faultfree = pd.read_csv(\"DatasetReducido/FaultFree_Training_reduced.csv\")\n",
    "\n",
    "print(\"FAULTY:\")\n",
    "print(df_faulty[\"fault_present\"].value_counts())\n",
    "print(\"Simulaciones únicas:\", df_faulty[\"simulationRun\"].nunique())\n",
    "\n",
    "print(\"\\nFAULTFREE:\")\n",
    "print(df_faultfree[\"fault_present\"].value_counts())\n",
    "print(\"Simulaciones únicas:\", df_faultfree[\"simulationRun\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09411d01-32f4-4b72-a448-c273163baa69",
   "metadata": {},
   "source": [
    "Al analizar los resultados podemos ver como las condiciones de los dataset son las correctas segun la reducción que hemos realizado. \n",
    "\n",
    "El dataset de Faulty Training, en la columna fault_present solo tiene el valor 1, esto indica que en este nuevo archivo solo hay simulaciones que tienen fallos, y cuenta con 10 simulaciones únicas.\n",
    "El dataset FaultFree Training solo tiene el valor 0 en la columna fault_present, y asi se verifica que el dataset solo tiene simulaciones (normales) sin fallos. En este caso son 20 simulaciones únicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44019b8d-61ba-4575-ae75-438cea9c4351",
   "metadata": {},
   "source": [
    "Una vez hemos realizado este proceso para los cuatro archivos del dataste TEP, obtenemos un conjunto de datos reducido, pero representativo y eficiente. Este dataset nuevo será el que usaremos para el resto del proyecto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
