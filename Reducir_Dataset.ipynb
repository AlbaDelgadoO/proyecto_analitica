{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80d88e8-b48a-425e-8167-b82acabc444b",
   "metadata": {},
   "source": [
    "# Reducción del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c9541-c376-4f3d-80e1-584c7f0f489e",
   "metadata": {},
   "source": [
    "En la fase 1 del proyecto (Identificación y estudio teórico de datos industriales) analizamos la estructura del dataset *Tennessee Eastman Process (TEP)*. Este esta compuesto por simulaciones industriales que representan tanto el comportamiento normal como los 20 distintos tipos de fallos.\n",
    "\n",
    "En ese análisis detallamos las variables que forman parte del proceso, comprobamos que no había valores nulos, y confirmamos que el volumen de datos del dataset era enorme (cientos de miles de registros), especialmente para los archivos de fallos.\n",
    "\n",
    "Este tamaño es ideal para realizar una investigación avanzada, pero para desarrollar nuestro proyecto, nos supone grandes limitaciones. Por ejemplo, usar el dataste completo afecta en los tiempos de carga de los archivos, retrasa el entrenamiento y dificulta la experimentación del usuario en los paneles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0599529b-413f-4df0-8a29-21023f73b4c0",
   "metadata": {},
   "source": [
    "Por estas razones, antes de avanzar con los siguientes pasos del proyecto, entre ellos realizar un análisis exploratorio más profundo, la ingeniería de características y creación de modelos, vamos a **reducir el dataset original**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d908170a-dd21-4c67-b554-632a6c3a3a88",
   "metadata": {},
   "source": [
    "#### Estrategia para la reducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596de332-6d71-45cd-8f6b-99dcfd100515",
   "metadata": {},
   "source": [
    "Para conservar el proceso industrial pero sin usar todas las simulaciones del dataset, vamos a usar un **muestreo estratificado** que se basa en simulaciones completas.\n",
    "\n",
    "Mantenemos todas la columnas del conjunto de datos original y los segmentos temporales (conservamos las simulaciones completas). Creamos una nueva variable binaria, *fault_present*,que nos puede resultar útil en proximas tareas. En cambio, para cada tipo de fallo seleccionamos 10 simulaciones completas, y para los archivos donde no tenemos fallos, seleccionames 20 simulaciones.\n",
    "\n",
    "De esta forma garantizamos que los datos reducidos mantienen la dimensionalidad original (las 52 variables del proceso) y reflejamos ambas condiciones, las normales y con los fallos simulados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f17c7-4427-471d-ba24-f25ca14375f5",
   "metadata": {},
   "source": [
    "### Faulty_Training y Faulty_Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6040a18-7927-46d7-9b23-c34032dc6f4e",
   "metadata": {},
   "source": [
    "Comenzamos reduciendo los archivos que tienen las simulaciones con fallos.\n",
    "\n",
    "Primero cargamos los archivos y convertimos las columnas numéricas a float, despues creamos una etiqueta para indicar si hay fallo o no. El siguiente paso es eleguir un número limitado de simulaciones por fallo y tambien algunas simulaciones sin fallo para generar los nuevos archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2671efee-df7f-459a-bd4d-2e96e486c0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Faulty Training reducido ---\n",
      "Archivo guardado en: C:\\Users\\miren\\Documents\\GitHub\\proyecto_analitica\\DatasetReducido\\Faulty_Training_reduced.csv\n",
      "Simulaciones: 10\n",
      "Muestras: 100000\n",
      "Columnas: ['faultNumber', 'simulationRun', 'sample', 'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11', 'fault_present']\n",
      "\n",
      "--- Faulty Testing reducido ---\n",
      "Archivo guardado en: C:\\Users\\miren\\Documents\\GitHub\\proyecto_analitica\\DatasetReducido\\Faulty_Testing_reduced.csv\n",
      "Simulaciones: 10\n",
      "Muestras: 192000\n",
      "Columnas: ['faultNumber', 'simulationRun', 'sample', 'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11', 'fault_present']\n",
      "\n",
      "Todos los datasets reducidos guardados en la carpeta DatasetReducido.\n"
     ]
    }
   ],
   "source": [
    "import pyreadr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_and_reduce(file_path, n_sims_per_fault=10, n_sims_faultfree=20):\n",
    "    \"\"\"\n",
    "    Carga un archivo .RData, convierte a float32 para ahorrar memoria,\n",
    "    aplica estratificación por tipo de fallo y devuelve un DataFrame reducido\n",
    "    manteniendo todas las columnas y sus contenidos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = pyreadr.read_r(file_path)\n",
    "        df_name = list(result.keys())[0]\n",
    "        df = result[df_name]\n",
    "\n",
    "        # Reducir tipo de dato (float64 -> float32)\n",
    "        for col in df.select_dtypes(include=\"float64\").columns:\n",
    "            df[col] = df[col].astype(\"float32\")\n",
    "\n",
    "        # Añadir etiqueta binaria (sin eliminar nada)\n",
    "        if 'faultNumber' in df.columns:\n",
    "            df['fault_present'] = np.where(df['faultNumber'] > 0, 1, 0)\n",
    "\n",
    "        # Detectar columna de simulación\n",
    "        sim_col = None\n",
    "        for candidate in ['simulationRun', 'simRun', 'run']:\n",
    "            if candidate in df.columns:\n",
    "                sim_col = candidate\n",
    "                break\n",
    "        if sim_col is None:\n",
    "            raise KeyError(\"No encontré columna de simulación en el dataset\")\n",
    "\n",
    "        # --- Estratificación ---\n",
    "        if 'faultNumber' in df.columns:\n",
    "            # Simulaciones con fallo\n",
    "            df_faulty = df[df['faultNumber'] > 0]\n",
    "            selected_faulty_sims = (\n",
    "                df_faulty.groupby('faultNumber')[sim_col]\n",
    "                .apply(lambda sims: sims.drop_duplicates().sample(\n",
    "                    n=min(n_sims_per_fault, len(sims)), random_state=42))\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            # Simulaciones sin fallo\n",
    "            df_faultfree = df[df['faultNumber'] == 0]\n",
    "            selected_faultfree_sims = (\n",
    "                df_faultfree[sim_col]\n",
    "                .drop_duplicates()\n",
    "                .sample(n=min(n_sims_faultfree, len(df_faultfree[sim_col].unique())), random_state=42)\n",
    "            )\n",
    "\n",
    "            # Combinar IDs de simulaciones seleccionadas\n",
    "            selected_sims = pd.concat([selected_faulty_sims, selected_faultfree_sims])\n",
    "            df_reduced = df[df[sim_col].isin(selected_sims)]\n",
    "        else:\n",
    "            # Caso FaultFree: solo muestreo de simulaciones\n",
    "            selected_sims = (\n",
    "                df[sim_col]\n",
    "                .drop_duplicates()\n",
    "                .sample(n=min(n_sims_faultfree, len(df[sim_col].unique())), random_state=42)\n",
    "            )\n",
    "            df_reduced = df[df[sim_col].isin(selected_sims)]\n",
    "\n",
    "        return df_reduced, sim_col\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar {file_path}: {e}\")\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "\n",
    "# Ejemplo de uso completo\n",
    "DATA_DIR = os.getcwd()\n",
    "DEST_DIR = os.path.join(DATA_DIR, \"DatasetReducido\")\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "\n",
    "file_paths = {\n",
    "    \"Faulty Training\": os.path.join(DATA_DIR, \"TEP_Faulty_Training.RData\"),\n",
    "    \"Faulty Testing\": os.path.join(DATA_DIR, \"TEP_Faulty_Testing.RData\"),\n",
    "}\n",
    "\n",
    "for name, path in file_paths.items():\n",
    "    df_reduced, sim_col = load_and_reduce(path)\n",
    "    if not df_reduced.empty:\n",
    "        out_file = os.path.join(DEST_DIR, f\"{name.replace(' ', '_')}_reduced.csv\")\n",
    "        df_reduced.to_csv(out_file, index=False)\n",
    "        print(f\"\\n--- {name} reducido ---\")\n",
    "        print(\"Archivo guardado en:\", out_file)\n",
    "        print(\"Simulaciones:\", df_reduced[sim_col].nunique())\n",
    "        print(\"Muestras:\", len(df_reduced))\n",
    "        print(\"Columnas:\", df_reduced.columns.tolist())\n",
    "\n",
    "print(\"\\nTodos los datasets reducidos guardados en la carpeta DatasetReducido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270c7d4-6ba5-4ffa-abe8-22bd5ec1d43f",
   "metadata": {},
   "source": [
    "### FaultFree_Training y FaultFree_Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16064c6b-b60d-47fb-b4e2-35a9b67b22e9",
   "metadata": {},
   "source": [
    "Para estos otros dos archivos que no contienen fallos usamos la misma lógica. Cargamos los datasets, convertimos los datos numéricos a float y eleguimos un número limitado de simulaciones. De esta forma creamos un dataset reducido conservando las columnas originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074c3b67-3ddb-482a-99af-2d59d7820b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FaultFree Training reducido ---\n",
      "Archivo guardado en: C:\\Users\\miren\\Documents\\GitHub\\proyecto_analitica\\DatasetReducido\\FaultFree_Training_reduced.csv\n",
      "Simulaciones: 20\n",
      "Muestras: 10000\n",
      "Columnas: ['faultNumber', 'simulationRun', 'sample', 'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11', 'fault_present']\n",
      "\n",
      "--- FaultFree Testing reducido ---\n",
      "Archivo guardado en: C:\\Users\\miren\\Documents\\GitHub\\proyecto_analitica\\DatasetReducido\\FaultFree_Testing_reduced.csv\n",
      "Simulaciones: 20\n",
      "Muestras: 19200\n",
      "Columnas: ['faultNumber', 'simulationRun', 'sample', 'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11', 'fault_present']\n",
      "\n",
      "Todos los datasets reducidos guardados en la carpeta DatasetReducido.\n"
     ]
    }
   ],
   "source": [
    "import pyreadr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_and_reduce(file_path, n_sims_per_fault=10, n_sims_faultfree=20):\n",
    "    \"\"\"\n",
    "    Carga un archivo .RData, convierte a float32 para ahorrar memoria,\n",
    "    aplica estratificación por tipo de fallo y devuelve un DataFrame reducido\n",
    "    manteniendo todas las columnas y sus contenidos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = pyreadr.read_r(file_path)\n",
    "        df_name = list(result.keys())[0]\n",
    "        df = result[df_name]\n",
    "\n",
    "        # Reducir tipo de dato (float64 -> float32)\n",
    "        for col in df.select_dtypes(include=\"float64\").columns:\n",
    "            df[col] = df[col].astype(\"float32\")\n",
    "\n",
    "        # Añadir etiqueta binaria (sin eliminar nada)\n",
    "        if 'faultNumber' in df.columns:\n",
    "            df['fault_present'] = np.where(df['faultNumber'] > 0, 1, 0)\n",
    "\n",
    "        # Detectar columna de simulación\n",
    "        sim_col = None\n",
    "        for candidate in ['simulationRun', 'simRun', 'run']:\n",
    "            if candidate in df.columns:\n",
    "                sim_col = candidate\n",
    "                break\n",
    "        if sim_col is None:\n",
    "            raise KeyError(\"No encontré columna de simulación en el dataset\")\n",
    "\n",
    "        # --- Estratificación ---\n",
    "        if 'faultNumber' in df.columns:\n",
    "            # Simulaciones con fallo\n",
    "            df_faulty = df[df['faultNumber'] > 0]\n",
    "            selected_faulty_sims = (\n",
    "                df_faulty.groupby('faultNumber')[sim_col]\n",
    "                .apply(lambda sims: sims.drop_duplicates().sample(\n",
    "                    n=min(n_sims_per_fault, len(sims)), random_state=42))\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            # Simulaciones sin fallo\n",
    "            df_faultfree = df[df['faultNumber'] == 0]\n",
    "            selected_faultfree_sims = (\n",
    "                df_faultfree[sim_col]\n",
    "                .drop_duplicates()\n",
    "                .sample(n=min(n_sims_faultfree, len(df_faultfree[sim_col].unique())), random_state=42)\n",
    "            )\n",
    "\n",
    "            # Combinar IDs de simulaciones seleccionadas\n",
    "            selected_sims = pd.concat([selected_faulty_sims, selected_faultfree_sims])\n",
    "            df_reduced = df[df[sim_col].isin(selected_sims)]\n",
    "        else:\n",
    "            # Caso FaultFree: solo muestreo de simulaciones\n",
    "            selected_sims = (\n",
    "                df[sim_col]\n",
    "                .drop_duplicates()\n",
    "                .sample(n=min(n_sims_faultfree, len(df[sim_col].unique())), random_state=42)\n",
    "            )\n",
    "            df_reduced = df[df[sim_col].isin(selected_sims)]\n",
    "\n",
    "        return df_reduced, sim_col\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar {file_path}: {e}\")\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Ejemplo de uso completo\n",
    "# ------------------------------\n",
    "DATA_DIR = os.getcwd()\n",
    "DEST_DIR = os.path.join(DATA_DIR, \"DatasetReducido\")\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "\n",
    "file_paths = {\n",
    "    \"FaultFree Training\": os.path.join(DATA_DIR, \"TEP_FaultFree_Training.RData\"),\n",
    "    \"FaultFree Testing\": os.path.join(DATA_DIR, \"TEP_FaultFree_Testing.RData\"),\n",
    "}\n",
    "\n",
    "for name, path in file_paths.items():\n",
    "    df_reduced, sim_col = load_and_reduce(path)\n",
    "    if not df_reduced.empty:\n",
    "        out_file = os.path.join(DEST_DIR, f\"{name.replace(' ', '_')}_reduced.csv\")\n",
    "        df_reduced.to_csv(out_file, index=False)\n",
    "        print(f\"\\n--- {name} reducido ---\")\n",
    "        print(\"Archivo guardado en:\", out_file)\n",
    "        print(\"Simulaciones:\", df_reduced[sim_col].nunique())\n",
    "        print(\"Muestras:\", len(df_reduced))\n",
    "        print(\"Columnas:\", df_reduced.columns.tolist())\n",
    "\n",
    "print(\"\\nTodos los datasets reducidos guardados en la carpeta DatasetReducido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44019b8d-61ba-4575-ae75-438cea9c4351",
   "metadata": {},
   "source": [
    "Una vez hemos realizado este proceso para los cuatro archivos del dataste TEP, obtenemos un conjunto de datos reducido, pero representativo y eficiente. Este dataset nuevo será el que usaremos para el resto del proyecto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
